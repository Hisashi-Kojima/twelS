# this file contains our base configuration.
version: '3'

services:
  python:
    build:
      context: .
      dockerfile: ./python/Dockerfile
    command: >
      sh -c '
        ./entrypoint.sh
        uwsgi /code/uwsgi/uwsgi.ini
        /bin/sh
      '
    container_name: $PYTHON_CONTAINER_NAME
    # Solrコンテナが立ち上がるのを待つために"sleep 10"とかを使用すると、
    # 端末の性能によって成功するかどうかが変わりうるので、
    # depends_onのconditionでservice_healthyを指定した。
    depends_on:
      solr1:
        condition: service_healthy
      solr2:
        condition: service_healthy
      solr3:
        condition: service_healthy
    environment:
      - PYTHONPATH=/code
    networks:
      - app_net
    volumes:
      - ./.env:/code/.env  # for django-environ
      - ./python/twels:/code/twels  # own packages
      - ./logs:/code/logs

  db:
    container_name: $DB_CONTAINER_NAME
    environment:
      MYSQL_ROOT_PASSWORD: $MY_ROOT_PASSWORD
      MYSQL_DATABASE: $MY_DB_NAME  # イメージの起動時に作成するデータベースの名前
      MYSQL_USER: hisashi
      MYSQL_PASSWORD: $MY_HISASHI_PASSWORD
    image: mysql:$MYSQL_VERSION
    networks:
      - app_net
    ports:
      # host port : container port
      # container_nameとコンテナ側のportを使ってmysqlコンテナと接続しているので，
      # host portは何でもよい．
      - 3306:3306
    volumes:
      - ./db/mysql:/var/lib/mysql  # 永続化
      - ./db/conf:/etc/mysql/conf.d  # custom configのマウント
      - ./db/logs:/var/log/mysql
      - ./sqls/initdb:/docker-entrypoint-initdb.d  # ここにマウントしたディレクトリにあるファイルは，mysqlディレクトリが空の場合，起動時に実行される

  init:
    # There are certain configuration files containing cluster wide configuration.
    # Since some of these are crucial for the cluster to function properly,
    # you may need to upload such files to ZooKeeper
    # before starting your Solr cluster for the first time.
    # Examples of such configuration files (not exhaustive) are security.json and clusterprops.json.

    # If you for example would like to enable authentication, 
    # you can push your security.json file to ZooKeeper with the bin/solr utility.

    # more info:
    # https://solr.apache.org/guide/solr/latest/deployment-guide/zookeeper-file-management.html#preparing-zookeeper-before-first-cluster-start
    
    # With SolrCloud, Solr configuration files are kept in ZooKeeper,
    # so upload the configuration files to ZooKeeper.
    command: >
      sh -c '
        /opt/solr/server/scripts/cloud-scripts/zkcli.sh -zkhost zoo1:2181 -cmd upconfig -confdir /code/solr-conf -confname myconf
        /opt/solr/bin/solr zk cp /code/security.json zk:/security.json -z zoo1:2181
        /opt/solr/bin/solr zk cp /opt/solr/server/solr/solr.xml zk:/solr.xml -z zoo1:2181
      '
    depends_on:
      - zoo1
      - zoo2
      - zoo3
    environment:
      - ZK_HOST=zoo1:2181,zoo2:2181,zoo3:2181
    image: solr:9.2.1
    networks:
      - app_net
    volumes:
      - ./java/solr-conf:/code/solr-conf
      - ./java/security.json:/code/security.json
      - ./java/solr-data/solr.xml:/opt/solr/server/solr/solr.xml

  solr1:
    build:
      args:
        - USER=${USER:?}
        - UID=${UID:?}
        - GROUP=${GROUP:?}
        - GID=${GID:?}
      context: .
      dockerfile: ./java/Dockerfile
    container_name: solr1
    depends_on:
      init:
        condition: service_completed_successfully
    environment:
      - ZK_HOST=zoo1:2181,zoo2:2181,zoo3:2181
    healthcheck:
      # solr:SolrRocks is initial_user:initial_password.
      # 'curl http://localhost:8983/api/collections' is used as ping.
      test: 'curl -f --user solr:SolrRocks http://localhost:8983/api/collections || exit 1'
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - app_net
    ports:
     - "8981:8983"
    volumes:
      - ./java/solr-data/log4j2.xml:/var/solr/log4j2.xml
      - ./java/solr-data/solr1:/var/solr/data
      - ./logs/solr/solr1:/var/solr/logs
      - ./java/backup:/opt/solr/server/backup  # share the drive with other solr containers to backup.

  solr2:
    build:
      args:
        - USER=${USER:?}
        - UID=${UID:?}
        - GROUP=${GROUP:?}
        - GID=${GID:?}
      context: .
      dockerfile: ./java/Dockerfile
    container_name: solr2
    depends_on:
      init:
        condition: service_completed_successfully
    environment:
      - ZK_HOST=zoo1:2181,zoo2:2181,zoo3:2181
    healthcheck:
      test: 'curl -f --user solr:SolrRocks http://localhost:8983/api/collections || exit 1'
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - app_net
    ports:
     - "8982:8983"
    volumes:
      - ./java/solr-data/solr2:/var/solr/data
      - ./logs/solr/solr2:/var/solr/logs
      - ./java/backup:/opt/solr/server/backup  # share the drive with other solr containers to backup.
    
  solr3:
    build:
      args:
        - USER=${USER:?}
        - UID=${UID:?}
        - GROUP=${GROUP:?}
        - GID=${GID:?}
      context: .
      dockerfile: ./java/Dockerfile
    container_name: solr3
    depends_on:
      init:
        condition: service_completed_successfully
    environment:
      - ZK_HOST=zoo1:2181,zoo2:2181,zoo3:2181
    healthcheck:
      test: 'curl -f --user solr:SolrRocks http://localhost:8983/api/collections || exit 1'
      interval: 5s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - app_net
    ports:
      - "8983:8983"
    volumes:
      - ./java/solr-data/solr3:/var/solr/data
      - ./logs/solr/solr3:/var/solr/logs
      - ./java/backup:/opt/solr/server/backup  # share the drive with other solr containers to backup.


  zoo1:
    container_name: zoo1
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
      ZOO_4LW_COMMANDS_WHITELIST: mntr, conf, ruok
      ZOO_CFG_EXTRA: "metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider metricsProvider.httpPort=7000 metricsProvider.exportJvmInfo=true"
    hostname: zoo1
    image: zookeeper:3.8.1
    networks:
      - app_net
    ports:
      - 2181:2181
      - 7001:7000
    restart: always
    volumes:
      - ./java/zoo-keeper/zoo1/data:/data
      - ./java/zoo-keeper/zoo1/datalog:/datalog

  zoo2:
    container_name: zoo2
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
      ZOO_4LW_COMMANDS_WHITELIST: mntr, conf, ruok
      ZOO_CFG_EXTRA: "metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider metricsProvider.httpPort=7000 metricsProvider.exportJvmInfo=true"
    hostname: zoo2
    image: zookeeper:3.8.1
    networks:
      - app_net
    ports:
      - 2182:2181
      - 7002:7000
    restart: always
    volumes:
      - ./java/zoo-keeper/zoo2/data:/data
      - ./java/zoo-keeper/zoo2/datalog:/datalog

  zoo3:
    container_name: zoo3
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
      ZOO_4LW_COMMANDS_WHITELIST: mntr, conf, ruok
      ZOO_CFG_EXTRA: "metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider metricsProvider.httpPort=7000 metricsProvider.exportJvmInfo=true"
    hostname: zoo3
    image: zookeeper:3.8.1
    networks:
      - app_net
    ports:
      - 2183:2181
      - 7003:7000
    restart: always
    volumes:
      - ./java/zoo-keeper/zoo3/data:/data
      - ./java/zoo-keeper/zoo3/datalog:/datalog


networks:
  app_net:
    driver: bridge